# -*- coding: utf-8 -*-
"""Lead_Scoring_assignment_final_1.2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NAMRkyIKasuFKHVb5N0h0FEIyd1AX0xD

## Lead Scoring Case Study

# Step 1 : Importing Data
"""

# Suppressing Warnings
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
pd.set_option("display.max_rows",1000)
import warnings
warnings.filterwarnings('ignore')

# Importing Pandas and NumPy
import pandas as pd, numpy as np

# Importing all datasets
lead_data = pd.read_csv("/content/Leads.csv")
lead_data.head()

# Let's see the type of each column
lead_data.info()

lead_data.describe()

lead_df=lead_data

lead_df['Do Not Email'].value_counts()

"""# Changing all the "Select" values to null and finding to drop columns """

lead_df['Lead Profile']=lead_df['Lead Profile'].replace('Select',np.NaN)
lead_df['Lead Profile'].isnull().value_counts()

lead_df['Specialization'].value_counts()
lead_df['Specialization']=lead_df['Specialization'].replace('Select',np.NaN)

temp=lead_data.isnull().mean().reset_index()
temp[0]=temp[0]*100
temp=temp.rename(columns={"index":"Column Name",0:"%Missing Values"})
temp=temp.sort_values(by=["%Missing Values"],ascending=False)
cols_to_drop=list(temp[temp["%Missing Values"]>30]["Column Name"])
cols_to_drop

"""# Imputing the Null values with mean and other Labels"""

lead_df.rename(columns = {'What is your current occupation':'Occupation'}, inplace = True)
lead_df['Occupation']=lead_df['Occupation'].fillna("Unknown")
lead_df['Occupation'].value_counts()

lead_df['TotalVisits'].isnull().value_counts()

lead_df['TotalVisits']=lead_df['TotalVisits'].fillna(lead_df['TotalVisits'].mean())

lead_df['Page Views Per Visit'].isnull().value_counts()

lead_df['How did you hear about X Education']=lead_df['How did you hear about X Education'].replace('Select',np.NaN)

lead_df['Page Views Per Visit'].value_counts()
lead_df['Page Views Per Visit']=lead_df['Page Views Per Visit'].fillna(lead_df['Page Views Per Visit'].mean())

#To check null values for all columns
temp=100*lead_df.isnull().mean()
temp



"""# Dropping Columns after cleaning data having null values > 35"""

temp=lead_data.isnull().mean().reset_index()
temp[0]=temp[0]*100
temp=temp.rename(columns={"index":"Column Name",0:"%Missing Values"})
temp=temp.sort_values(by=["%Missing Values"],ascending=False)
cols_to_drop=list(temp[temp["%Missing Values"]>35]["Column Name"])
cols_to_drop=cols_to_drop +["Country","City","Last Activity","Prospect ID"]
cols_to_drop

# New df1 after dropping the columns
lead_df=lead_data.drop(cols_to_drop,axis=1)
lead_df.shape

lead_df['Lead Source'].isnull().value_counts()
lead_df['Lead Source']=lead_df['Lead Source'].fillna('Unknown')

lead_df.info()

lead_df['A free copy of Mastering The Interview'].value_counts()
lead_df.rename(columns = {'A free copy of Mastering The Interview':'Free Book'}, inplace = True)

"""# Mapping all the columns having " Yes/No" to "1/0"
"""

# List of variables to map

varlist =  ['Do Not Email','Do Not Call','Free Book','X Education Forums','Newspaper','Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses','Update me on Supply Chain Content','Get updates on DM Content','I agree to pay the amount through cheque']

# Defining the map function
def binary_map(x):
    return x.map({'Yes': 1, "No": 0})

# Applying the function to the housing list
lead_df[varlist] = lead_df[varlist].apply(binary_map)

plt.figure(figsize=(20,10))
sns.heatmap(lead_df.corr(),annot=True)

lead_df['Get updates on DM Content'].value_counts()

"""# Selecting the columns for analysis as per the requirement having correlation >0.3"""

lead_df.head()

cat=['Converted','Lead Origin','Lead Source','Occupation','Last Notable Activity','Do Not Email','Do Not Call','Free Book','X Education Forums']
coninous=['Total Visits','Total Time Spent on Website','Page Views Per Visit']
mycolumns=['Lead Number','Converted','Lead Origin','Lead Source','TotalVisits','Total Time Spent on Website','Occupation','Last Notable Activity','Page Views Per Visit','Do Not Email','Do Not Call','Free Book','X Education Forums']
mylead_df=lead_df
lead_df=lead_df[mycolumns]



#dataframe having the below columns selected for for final modelling
lead_df.isnull().sum()



for i in cat:
  print("Countplot of",i)
  sns.countplot(lead_df[i])
  plt.xticks(rotation=90)
  plt.show()

# New df1 after dropping the columns as these two columns are not having significant unique vales
lead_df=lead_df.drop(['X Education Forums','Do Not Call'],axis=1)
lead_df.shape

other_classes = lead_df['Lead Source'].value_counts()[lead_df['Lead Source'].value_counts() < 10].index

lead_df['Lead Source'][lead_df['Lead Source'].isin(other_classes)] = 'OTHER'

lead_df['Lead Source'].value_counts()

# Creating a dummy variable for some of the categorical variables and dropping the first one.
dummy1 = pd.get_dummies(lead_df[['Lead Origin','Occupation','Last Notable Activity']], drop_first=True)

# Adding the results to the master dataframe
lead_df = pd.concat([lead_df, dummy1], axis=1)
lead_df.head()

# We have created dummies for the below variables, so we can drop them
lead_df = lead_df.drop(['Lead Origin','Occupation','Last Notable Activity','Occupation','Lead Source'], 1)

lead_df.isnull().sum()

"""# Checking for Outliers"""

# Checking for outliers in the continuous variables
num_lead = lead_df[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']]
cont=['TotalVisits','Total Time Spent on Website','Page Views Per Visit']

for i in cont:    
    q1=lead_df[i].describe()["25%"]
    q3=lead_df[i].describe()["75%"]
    iqr=q3-q1 #interquartile range
    lower_bound=q1-1.5*iqr
    upper_bound=q3+1.5*iqr
    lead_df[i]=np.where(lead_df[i]>upper_bound,upper_bound,lead_df[i])
    lead_df[i]=np.where(lead_df[i]<lower_bound,lower_bound,lead_df[i])

# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%
num_lead.describe(percentiles=[.25, .5, .75, .90, .95, .99])

sns.boxplot(lead_df['TotalVisits'])
plt.show()
sns.boxplot(lead_df['Total Time Spent on Website'])
plt.show()





"""# Machine Modelling: Dividing data as Train/Test"""

from sklearn.model_selection import train_test_split

# Putting feature variable to X
X = lead_df.drop(['Converted','Lead Number'], axis=1)

X.head()

# Putting response variable to y
y = lead_df['Converted']

y.head()

# Splitting the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaler = StandardScaler()

X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])

X_train.head()

import statsmodels.api as sm
# Logistic regression model
logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())
logm1.fit().summary()

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
from sklearn.feature_selection import RFE
rfe = RFE(logreg,n_features_to_select=15, step=1)             # running RFE with 13 variables as output
rfe = rfe.fit(X_train, y_train)

list(zip(X_train.columns, rfe.support_, rfe.ranking_))

col = X_train.columns[rfe.support_]

X_train.columns[~rfe.support_]

X_train_sm = sm.add_constant(X_train[col])
logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()

# Dropping "Occupation_Housewife" as p>0.05
col = col.drop('Occupation_Housewife', 1)

# Let's re-run the model using the selected variables
X_train_sm = sm.add_constant(X_train[col])
logm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm3.fit()
res.summary()



#Dropping "Last Notable Activity_Resubscribed to emails" as p>0.05
col = col.drop('Last Notable Activity_Resubscribed to emails', 1)

# Let's re-run the model using the selected variables
X_train_sm = sm.add_constant(X_train[col])
logm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm3.fit()
res.summary()

#Dropping "Last Notable Activity_Page Visited on Website" as p>0.05
col = col.drop('Last Notable Activity_Page Visited on Website', 1)
# Let's re-run the model using the selected variables
X_train_sm = sm.add_constant(X_train[col])
logm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm3.fit()
res.summary()

#Dropping "Last Notable Activity_Email Link Clicked" as p>0.05
col = col.drop('Last Notable Activity_Email Link Clicked', 1)
# Let's re-run the model using the selected variables
X_train_sm = sm.add_constant(X_train[col])
logm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm3.fit()
res.summary()

"""## The above model is the final model"""



# Getting the predicted values on the train set
y_train_pred = res.predict(X_train_sm)
y_train_pred[:10]



y_train_pred = y_train_pred.values.reshape(-1)
y_train_pred[:10]

y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Convert_prob':y_train_pred})
y_train_pred_final['Lead Number'] = y_train.index
y_train_pred_final.head()

y_train_pred_final['predicted'] = y_train_pred_final.Convert_prob.map(lambda x: 1 if x > 0.5 else 0)
y_train_pred_final['LeadScore']=y_train_pred_final.Convert_prob.map(lambda x:x*100)


# Let's see the head
y_train_pred_final.head()



from sklearn import metrics
# Confusion matrix 
confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )
print(confusion)

# Let's check the overall accuracy.
print(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))

# Check for the VIF values of the feature variables. 
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train[col].columns
vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""# Model beyond Simple Accuracy"""

TP = confusion[1,1] # true positive 
TN = confusion[0,0] # true negatives
FP = confusion[0,1] # false positives
FN = confusion[1,0] # false negatives

# Let's see the sensitivity of our logistic regression model
lead_sensitvity=TP / float(TP+FN)
# Let us calculate specificity
lead_specificity=TN / float(TN+FP)
print('Sensitivity =',lead_sensitvity)
print('Specificity =',lead_specificity)

# Calculate false postive rate - predicting churn when customer does not have churned
print(FP/ float(TN+FP))
# positive predictive value 
print (TP / float(TP+FP))
# Negative predictive value
print (TN / float(TN+ FN))

"""# Plotting the ROC Curve"""

def draw_roc( actual, probs ):
    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,
                                              drop_intermediate = False )
    auc_score = metrics.roc_auc_score( actual, probs )
    plt.figure(figsize=(5, 5))
    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()

    return None

fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Convert_prob, drop_intermediate = False )

draw_roc(y_train_pred_final.Converted, y_train_pred_final.Convert_prob)

"""# Finding the Optimal Cut-off"""

# Let's create columns with different probability cutoffs 
numbers = [float(x)/10 for x in range(10)]
for i in numbers:
    y_train_pred_final[i]= y_train_pred_final.Convert_prob.map(lambda x: 1 if x > i else 0)
y_train_pred_final.head()

# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.
cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])
from sklearn.metrics import confusion_matrix

# TP = confusion[1,1] # true positive 
# TN = confusion[0,0] # true negatives
# FP = confusion[0,1] # false positives
# FN = confusion[1,0] # false negatives

num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]
for i in num:
    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )
    total1=sum(sum(cm1))
    accuracy = (cm1[0,0]+cm1[1,1])/total1
    
    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])
    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])
    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]
print(cutoff_df)

# Let's plot accuracy sensitivity and specificity for various probabilities.
cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])
plt.grid()
plt.show()



"""# From the graph, will consider cut-off of 0.35"""

y_train_pred_final['final_predicted'] = y_train_pred_final.Convert_prob.map( lambda x: 1 if x > 0.35 else 0)

# Let's check the overall accuracy.
metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)

confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )
confusion2

TP = confusion2[1,1] # true positive 
TN = confusion2[0,0] # true negatives
FP = confusion2[0,1] # false positives
FN = confusion2[1,0] # false negatives

# Let's see the sensitivity of our logistic regression model
lead_sensitvity=TP / float(TP+FN)
# Let us calculate specificity
lead_specificity=TN / float(TN+FP)
print('Sensitivity =',lead_sensitvity)
print('Specificity =',lead_specificity)

# Positive predictive value 
print ('Positive predictive value',TP / float(TP+FP))
TN / float(TN+FP)# Negative predictive value
print ('Negative predictive value',TN / float(TN+ FN))

confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )
confusion

print('Precision =',confusion[1,1]/(confusion[0,1]+confusion[1,1]))
print('Recall=',confusion[1,1]/(confusion[1,0]+confusion[1,1]))

from sklearn.metrics import precision_score, recall_score
print('Precion from sklearn',precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted))
print('Reacll from sklearn',recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted))

from sklearn.metrics import precision_recall_curve
p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Convert_prob)

plt.plot(thresholds, p[:-1], "g-")
plt.plot(thresholds, r[:-1], "r-")
plt.show()

"""# Making predictions on the test set"""

X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])
X_test = X_test[col]

X_test.head()

X_test_sm = sm.add_constant(X_test)

y_test_pred = res.predict(X_test_sm)

# Converting y_pred to a dataframe which is an array
y_pred_1 = pd.DataFrame(y_test_pred)

# Converting y_test to dataframe
y_test_df = pd.DataFrame(y_test)

# Putting CustID to index
y_test_df['Lead Number'] = y_test_df.index

# Removing index for both dataframes to append them side by side 
y_pred_1.reset_index(drop=True, inplace=True)
y_test_df.reset_index(drop=True, inplace=True)

# Appending y_test_df and y_pred_1
y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)

# Appending y_test_df and y_pred_1
y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)

y_pred_final= y_pred_final.rename(columns={ 0 : 'Convert_prob'})

y_pred_final = y_pred_final.reindex(['Lead Number','Converted','Convert_prob'],axis=1)

y_pred_final['final_predicted'] = y_pred_final.Convert_prob.map(lambda x: 1 if x > 0.42 else 0)

# Let's check the overall accuracy.
print('Accuracy= ',metrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted))

confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )
confusion2

TP = confusion2[1,1] # true positive 
TN = confusion2[0,0] # true negatives
FP = confusion2[0,1] # false positives
FN = confusion2[1,0] # false negatives

# Let's see the sensitivity of our logistic regression model
TP / float(TP+FN)

# Let us calculate specificity
TN / float(TN+FP)







